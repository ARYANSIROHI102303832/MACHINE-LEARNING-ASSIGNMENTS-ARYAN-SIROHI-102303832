{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOsbsQ5eYNbYA1IOkwEzsG2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IKzV_XzjaHbn","executionInfo":{"status":"ok","timestamp":1763813016734,"user_tz":-330,"elapsed":338,"user":{"displayName":"ARYAN SIROHI","userId":"08584800987310056893"}},"outputId":"0dd1e860-77d1-4740-eada-ab1f47c09675"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import r2_score\n","import itertools\n","\n","# -------------------------------------------\n","# Step 1: Generate dataset with correlated features\n","# -------------------------------------------\n","np.random.seed(0)\n","n = 400\n","\n","X1 = np.random.randn(n)\n","X2 = X1 + np.random.normal(0, 0.1, n)\n","X3 = X1 + X2 + np.random.normal(0, 0.1, n)\n","X4 = X1*2 + np.random.normal(0, 0.2, n)\n","X5 = X2*3 + np.random.normal(0, 0.2, n)\n","X6 = X3*2 + np.random.normal(0, 0.1, n)\n","X7 = X1 + X2 + X3 + X4 + np.random.normal(0, 0.5, n)\n","\n","X = np.vstack([X1,X2,X3,X4,X5,X6,X7]).T\n","true_w = np.array([5,3,2,4,1,-2,3])\n","y = X.dot(true_w) + np.random.normal(0, 3, n)\n","\n","# Add bias column\n","X_b = np.c_[np.ones((n,1)), X]\n","\n","# -------------------------------------------\n","# Step 2: Ridge Regression (Gradient Descent)\n","# -------------------------------------------\n","def ridge_gradient_descent(X, y, lr, lam, epochs=500):\n","    m, n = X.shape\n","    w = np.zeros(n)\n","\n","    for _ in range(epochs):\n","        y_pred = X.dot(w)\n","        grad = (1/m) * X.T.dot(y_pred - y) + (lam/m)*np.r_[0, w[1:]]\n","        w -= lr * grad\n","        # Add a check for NaN in weights after update to prevent further propagation\n","        if np.isnan(w).any():\n","            break\n","    return w\n","\n","# Hyperparameter search\n","# Removed learning rates 0.1 and 1 to prevent RuntimeWarnings due to numerical instability\n","learning_rates = [0.0001,0.001,0.01]\n","lambdas = [1e-15,1e-10,1e-5,1e-3,0,1,10,20]\n","\n","best_params = None\n","best_score = -np.inf\n","best_cost = np.inf\n","\n","for lr, lam in itertools.product(learning_rates, lambdas):\n","    w = ridge_gradient_descent(X_b, y, lr, lam)\n","    y_pred = X_b.dot(w)\n","\n","    # Check for NaNs in y_pred before calculating r2_score and cost\n","    if np.isnan(y_pred).any():\n","        # If NaNs are present, this combination of hyperparameters is bad.\n","        # Assign a very low R2 score and a very high cost to discard it.\n","        r2 = -np.inf\n","        cost = np.inf\n","    else:\n","        cost = np.mean((y - y_pred)**2) + lam * np.sum(w[1:]**2)\n","        r2 = r2_score(y, y_pred)\n","\n","    if r2 > best_score:\n","        best_score = r2\n","        best_cost = cost\n","        best_params = (lr, lam, w)\n","\n","print(\"Best Learning Rate:\", best_params[0])\n","print(\"Best Lambda:\", best_params[1])\n","print(\"Best R²:\", best_score)\n","print(\"Minimum Cost:\", best_cost)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Learning Rate: 0.01\n","Best Lambda: 1e-15\n","Best R²: 0.9913571122179273\n","Minimum Cost: 9.157879211469126\n"]}]},{"cell_type":"code","source":["\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","from sklearn.metrics import r2_score, mean_squared_error\n","df = pd.read_csv(\"/content/Hitters.csv\")\n","# Remove missing values\n","df = df.dropna()\n","\n","# Convert categorical columns\n","df = pd.get_dummies(df, drop_first=True)\n","\n","# Features/Target\n","X = df.drop(\"Salary\", axis=1)\n","y = df[\"Salary\"]\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Scaling\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","\n","# -------------------------------------------\n","# Fit models\n","# -------------------------------------------\n","lr = LinearRegression()\n","ridge = Ridge(alpha=0.5748)\n","lasso = Lasso(alpha=0.5748)\n","\n","lr.fit(X_train, y_train)\n","ridge.fit(X_train, y_train)\n","lasso.fit(X_train, y_train)\n","\n","# Predictions\n","lr_pred = lr.predict(X_test)\n","ridge_pred = ridge.predict(X_test)\n","lasso_pred = lasso.predict(X_test)\n","\n","# Evaluation\n","print(\"Linear Reg R2:\", r2_score(y_test, lr_pred))\n","print(\"Ridge Reg R2:\", r2_score(y_test, ridge_pred))\n","print(\"Lasso Reg R2:\", r2_score(y_test, lasso_pred))\n","\n","print(\"\\nMSE Scores:\")\n","print(\"Linear:\", mean_squared_error(y_test, lr_pred))\n","print(\"Ridge:\", mean_squared_error(y_test, ridge_pred))\n","print(\"Lasso:\", mean_squared_error(y_test, lasso_pred))\n","\n","print(\"\\nBest Model:\",\n","      \"Ridge (handles multicollinearity best)\"\n","      if r2_score(y_test, ridge_pred) > r2_score(y_test, lr_pred) else \"Linear\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cEgZoZ5qaLB6","executionInfo":{"status":"ok","timestamp":1763813016768,"user_tz":-330,"elapsed":29,"user":{"displayName":"ARYAN SIROHI","userId":"08584800987310056893"}},"outputId":"f54012ab-cf58-434f-dc64-6f72160ec49f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear Reg R2: 0.290745185579814\n","Ridge Reg R2: 0.30003596988293446\n","Lasso Reg R2: 0.29928590166965496\n","\n","MSE Scores:\n","Linear: 128284.34549672354\n","Ridge: 126603.90264424692\n","Lasso: 126739.56899132291\n","\n","Best Model: Ridge (handles multicollinearity best)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.185e+04, tolerance: 4.367e+03\n","  model = cd_fast.enet_coordinate_descent(\n"]}]},{"cell_type":"code","source":["from sklearn.datasets import fetch_openml\n","from sklearn.linear_model import RidgeCV, LassoCV\n","\n","# Load Boston dataset\n","boston = fetch_openml(\"boston\", as_frame=True)\n","X = boston.data\n","y = boston.target\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Scale\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","\n","# RidgeCV\n","ridge_cv = RidgeCV(alphas=[0.1,1,10])\n","ridge_cv.fit(X_train, y_train)\n","\n","# LassoCV\n","lasso_cv = LassoCV(cv=5)\n","lasso_cv.fit(X_train, y_train)\n","\n","print(\"Best Ridge Alpha:\", ridge_cv.alpha_)\n","print(\"Ridge R2:\", ridge_cv.score(X_test, y_test))\n","\n","print(\"Best Lasso Alpha:\", lasso_cv.alpha_)\n","print(\"Lasso R2:\", lasso_cv.score(X_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FE2YmMtgbZ9f","executionInfo":{"status":"ok","timestamp":1763813016935,"user_tz":-330,"elapsed":162,"user":{"displayName":"ARYAN SIROHI","userId":"08584800987310056893"}},"outputId":"649daf00-0df7-4f81-cb5e-c9cca07f7654"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Ridge Alpha: 10.0\n","Ridge R2: 0.6659677905050292\n","Best Lasso Alpha: 0.006863892263379668\n","Lasso R2: 0.6683883969336302\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name boston exist. Versions may be fundamentally different, returning version 1. Available versions:\n","- version 1, status: active\n","  url: https://www.openml.org/search?type=data&id=531\n","- version 2, status: active\n","  url: https://www.openml.org/search?type=data&id=853\n","\n","  warn(warning_msg)\n"]}]},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","import numpy as np\n","\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","m, n = X.shape\n","\n","# Add bias\n","X = np.c_[np.ones(m), X]\n","\n","# Sigmoid\n","def sigmoid(z):\n","    return 1/(1+np.exp(-z))\n","\n","# Logistic Regression for one class\n","def train_binary(X, y, lr=0.1, epochs=2000):\n","    m, n = X.shape\n","    w = np.zeros(n)\n","    for _ in range(epochs):\n","        z = X.dot(w)\n","        h = sigmoid(z)\n","        gradient = (1/m) * X.T.dot(h - y)\n","        w -= lr * gradient\n","    return w\n","\n","# One-vs-rest\n","weights = []\n","classes = np.unique(y)\n","\n","for c in classes:\n","    y_binary = (y == c).astype(int)\n","    w = train_binary(X, y_binary)\n","    weights.append(w)\n","\n","weights = np.array(weights)\n","\n","# Prediction\n","def predict(X):\n","    scores = X.dot(weights.T)\n","    return np.argmax(scores, axis=1)\n","\n","# Accuracy\n","y_pred = predict(X)\n","print(\"Training Accuracy:\", np.mean(y_pred == y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctLhd_RKbmTE","executionInfo":{"status":"ok","timestamp":1763813016963,"user_tz":-330,"elapsed":24,"user":{"displayName":"ARYAN SIROHI","userId":"08584800987310056893"}},"outputId":"872c377f-287f-4b93-f4d0-6405406c17cc"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy: 0.96\n"]}]}]}